# -*- coding: utf-8 -*-
"""Augmented Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BIzmfvuDrX-xuPsUUL6MBO9Oywv6skzj
"""

pip install pandas numpy scikit-learn xgboost lightgbm catboost scipy

!pip install xgboost

"""Data augmentation gausian noise"""

import pandas as pd
import numpy as np

df = pd.read_csv('Finaldataset9.csv')
columns_to_augment_with_noise = ['floor_area', 'storeys', 'cost_rebased']
augmented_df = df.copy()
augmentation_factor = 2

for _ in range(augmentation_factor):
    temp_noisy_df = df.copy()
    for col in columns_to_augment_with_noise:
        noise_scale = 0.01 * df[col].std()
        if noise_scale == 0:
            noise_scale = 0.01
        noise = np.random.normal(loc=0, scale=noise_scale, size=len(df))
        temp_noisy_df[col] = df[col] + noise
        temp_noisy_df[col] = temp_noisy_df[col].apply(lambda x: max(x, 0))
    augmented_df = pd.concat([augmented_df, temp_noisy_df], ignore_index=True)

print(f"Original dataset size: {len(df)}")
print(f"Augmented dataset size: {len(augmented_df)}")

output_filename = 'augmented_dataset_minimal_processing.csv'
augmented_df.to_csv(output_filename, index=False)

"""ML Models"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

INPUT_COLS = ['type_of_work', 'building_function_code', 'floor_area', 'main_construction', 'storeys']
TARGET_COL = 'cost_rebased'

MODELS = {
    "Linear Regression": LinearRegression(),
    "Support Vector Regressor": SVR(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "XGBoost": XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42, n_jobs=-1),
    "K-Nearest Neighbors": KNeighborsRegressor(),
    "Multi-Layer Perceptron": MLPRegressor(random_state=42, max_iter=1000)
}

def evaluate_models(df):
    original_df = df.iloc[:2835]
    augmented_only_df = df.iloc[2835:]

    X_train = augmented_only_df[INPUT_COLS]
    y_train = augmented_only_df[TARGET_COL]

    X_test = original_df[INPUT_COLS]
    y_test = original_df[TARGET_COL]

    print(f"\nTraining on {len(X_train)} augmented rows, Testing on {len(X_test)} original rows")

    for name, model in MODELS.items():
        print(f"\n--- {name} ---")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)

        print(f"R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}")

# Main function
def main():
    try:
        df = pd.read_csv('/content/augmented_dataset_final.csv')
        evaluate_models(df)

    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()

"""Tree based ML models + ensemble stacking"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

INPUT_COLS = ['type_of_work', 'building_function_code', 'floor_area', 'main_construction', 'storeys']
TARGET_COL = 'cost_rebased'

MODELS = {
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "XGBoost": XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42, n_jobs=-1),
    "LightGBM": LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    "CatBoost": CatBoostRegressor(verbose=0, random_state=42)
}

def evaluate_models(df):
    original_df = df.iloc[:2835]
    augmented_only_df = df.iloc[2835:]

    X_train = augmented_only_df[INPUT_COLS]
    y_train = augmented_only_df[TARGET_COL]

    X_test = original_df[INPUT_COLS]
    y_test = original_df[TARGET_COL]

    print(f"\nTraining on {len(X_train)} augmented rows, Testing on {len(X_test)} original rows")

    for name, model in MODELS.items():
        print(f"\n--- {name} ---")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)

        print(f"R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}")

    print(f"\n--- Stacking Ensemble ---")
    stack_model = StackingRegressor(
        estimators=[
            ('rf', MODELS["Random Forest"]),
            ('gb', MODELS["Gradient Boosting"]),
            ('xgb', MODELS["XGBoost"]),
            ('lgbm', MODELS["LightGBM"]),
            ('cat', MODELS["CatBoost"])
        ],
        final_estimator=LinearRegression(),
        n_jobs=-1
    )

    stack_model.fit(X_train, y_train)
    y_pred = stack_model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    print(f"R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}")

def main():
    try:
        df = pd.read_csv('/content/augmented_dataset_final.csv')
        evaluate_models(df)

    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()

"""Hypertuned ML models"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
import warnings

warnings.filterwarnings('ignore')

ORIGINAL_INPUT_COLS = ['type_of_work', 'building_function_code', 'floor_area', 'main_construction', 'storeys']
TARGET_COL = 'cost_rebased'

BASE_MODELS = {
    "Random Forest": RandomForestRegressor(random_state=42, n_jobs=-1),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "XGBoost": XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1),
    "LightGBM": LGBMRegressor(random_state=42, n_jobs=-1),
    "CatBoost": CatBoostRegressor(verbose=0, random_state=42)
}

PARAM_DISTRIBUTIONS = {
    "Random Forest": {
        'n_estimators': randint(100, 500),
        'max_depth': randint(5, 20),
        'min_samples_split': randint(2, 10),
        'min_samples_leaf': randint(1, 5)
    },
    "XGBoost": {
        'n_estimators': randint(100, 500),
        'learning_rate': uniform(0.01, 0.2),
        'max_depth': randint(3, 10),
        'subsample': uniform(0.6, 0.4),
        'colsample_bytree': uniform(0.6, 0.4)
    }
}

def feature_engineer_no_leakage(data_df):
    df_copy = data_df.copy()
    df_copy['floor_area_per_storey'] = df_copy['floor_area'] / df_copy['storeys']
    df_copy.loc[df_copy['storeys'] == 0, 'floor_area_per_storey'] = 0
    df_copy['floor_area_per_storey'].replace([np.inf, -np.inf], 0, inplace=True)
    df_copy['floor_area_per_storey'] = df_copy['floor_area_per_storey'].fillna(0)
    df_copy['floor_area_x_storeys'] = df_copy['floor_area'] * df_copy['storeys']
    df_copy['floor_area_squared'] = df_copy['floor_area']**2
    return df_copy

def evaluate_models_novel_approach(df):
    required_cols = ORIGINAL_INPUT_COLS + [TARGET_COL]
    for col in required_cols:
        if col not in df.columns:
            print(f"Error: Required column '{col}' not found in the dataset. Please check your CSV.")
            return

    original_df = df.iloc[:2835]
    augmented_only_df = df.iloc[2835:]

    X_train_initial = augmented_only_df[ORIGINAL_INPUT_COLS].copy()
    y_train = augmented_only_df[TARGET_COL].copy()

    X_test_initial = original_df[ORIGINAL_INPUT_COLS].copy()
    y_test = original_df[TARGET_COL].copy()

    print(f"\nTraining on {len(X_train_initial)} augmented rows, Testing on {len(X_test_initial)} original rows")

    X_train_fe = feature_engineer_no_leakage(X_train_initial)
    X_test_fe = feature_engineer_no_leakage(X_test_initial)

    categorical_features = ['type_of_work', 'building_function_code', 'main_construction']
    numerical_features = ['floor_area', 'storeys', 'floor_area_per_storey', 'floor_area_x_storeys', 'floor_area_squared']

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
        ],
        remainder='passthrough'
    )

    tuned_models = {}
    print("\n--- Starting Hyperparameter Tuning for Base Models ---")
    for name, model in BASE_MODELS.items():
        model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', model)])
        if name in PARAM_DISTRIBUTIONS:
            print(f"  Tuning {name}...")
            random_search = RandomizedSearchCV(
                estimator=model_pipeline,
                param_distributions={f'regressor__{k}': v for k, v in PARAM_DISTRIBUTIONS[name].items()},
                n_iter=10,
                cv=2,
                verbose=0,
                random_state=42,
                n_jobs=-1,
                scoring='r2'
            )
            random_search.fit(X_train_fe, y_train)
            tuned_models[name] = random_search.best_estimator_
            print(f"  Best params for {name}: {random_search.best_params_}")
            print(f"  Best R² for {name} (on CV): {random_search.best_score_:.4f}")
        else:
            print(f"  Skipping tuning for {name}, using default parameters with preprocessor.")
            model_pipeline.fit(X_train_fe, y_train)
            tuned_models[name] = model_pipeline
    print("--- Hyperparameter Tuning Complete ---")

    print("\n--- Evaluating Tuned Individual Models on Original Test Set ---")
    tuned_preds = {}
    for name, model_pipeline in tuned_models.items():
        print(f"\n--- {name} (Tuned) ---")
        y_pred = model_pipeline.predict(X_test_fe)
        tuned_preds[name] = y_pred
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        print(f"Test R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}")

    print(f"\n--- Building and Evaluating Advanced Stacking Ensemble (with Tuned Models) ---")

    stack_estimators = []
    for name, pipeline in tuned_models.items():
        stack_estimators.append((name.lower().replace(" ", "_"), pipeline))

    meta_learner = XGBRegressor(objective='reg:squarederror', n_estimators=200, learning_rate=0.05, random_state=42, n_jobs=-1)

    stack_model = StackingRegressor(
        estimators=stack_estimators,
        final_estimator=meta_learner,
        cv=3,
        n_jobs=-1,
        passthrough=True
    )

    stack_model.fit(X_train_fe, y_train)
    y_pred = stack_model.predict(X_test_fe)

    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    print(f"Final Stacking Ensemble Results (Tuned Models & XGBoost Meta-Learner):")
    print(f"Test R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}")

    print("\n--- Weighted Averaging Ensemble (Conceptual) ---")
    print("For a truly robust weighted averaging ensemble, weights should be determined using a separate validation set.")
    print("Here, we demonstrate a simple average of predictions from the tuned models as an example.")

    if tuned_preds:
        avg_preds = np.mean(list(tuned_preds.values()), axis=0)
        r2_avg = r2_score(y_test, avg_preds)
        rmse_avg = np.sqrt(mean_squared_error(y_test, avg_preds))
        mae_avg = mean_absolute_error(y_test, avg_preds)
        print(f"Simple Averaging Ensemble (Tuned Models):")
        print(f"Test R²: {r2_avg:.4f}, RMSE: {rmse_avg:.4f}, MAE: {mae_avg:.4f}")

"""Hybrid stacking model"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge
from catboost import CatBoostRegressor
from xgboost import XGBRegressor
import matplotlib.pyplot as plt

df = pd.read_csv("augmented_dataset_final.csv")

df.drop(columns=['contract_contract_sum', 'cost_increment'], inplace=True)

X = df.drop(columns=['cost_rebased'])
y = df['cost_rebased']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


catboost = CatBoostRegressor(
    iterations=1000,
    depth=6,
    learning_rate=0.05,
    loss_function='RMSE',
    verbose=0,
    random_state=42
)
catboost.fit(X_train, y_train)

xgb = XGBRegressor(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    objective='reg:squarederror',
    random_state=42
)
xgb.fit(X_train, y_train)

mlp = MLPRegressor(
    hidden_layer_sizes=(128, 64, 32),
    activation='relu',
    solver='adam',
    alpha=0.001,
    learning_rate='adaptive',
    max_iter=500,
    random_state=42
)
mlp.fit(X_train_scaled, y_train)


estimators = [
    ('cat', catboost),
    ('xgb', xgb),
    ('mlp', MLPRegressor(
        hidden_layer_sizes=(128, 64, 32),
        activation='relu',
        solver='adam',
        alpha=0.001,
        learning_rate='adaptive',
        max_iter=500,
        random_state=42
    ))
]

stack_model = StackingRegressor(
    estimators=estimators,
    final_estimator=Ridge(alpha=1.0),
    cv=5,
    passthrough=True,
    n_jobs=-1
)

stack_model.fit(X_train_scaled, y_train)
y_pred = stack_model.predict(X_test_scaled)


r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print("--- Hybrid Stacking Ensemble Results ---")
print(f"R² Score: {r2:.4f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")


plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5, color='teal')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.title(f"Predicted vs Actual (R²: {r2:.4f})")
plt.xlabel("Actual Cost Rebased")
plt.ylabel("Predicted Cost Rebased")
plt.grid(True)
plt.tight_layout()
plt.show()

"""Neural Network Hybrid Model"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from catboost import CatBoostRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf


df = pd.read_csv('/content/augmented_dataset_final.csv')

df.dropna(inplace=True)


target = 'cost_rebased'
features = df.columns.drop(target)

X = df[features]
y = df[target]

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


xgb = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, subsample=0.8, random_state=42)
xgb.fit(X_train, y_train)
xgb_train_features = xgb.predict(X_train).reshape(-1, 1)
xgb_test_features = xgb.predict(X_test).reshape(-1, 1)


cat = CatBoostRegressor(verbose=0, iterations=300, learning_rate=0.05, depth=6, random_state=42)
cat.fit(X_train, y_train)
cat_train_features = cat.predict(X_train).reshape(-1, 1)
cat_test_features = cat.predict(X_test).reshape(-1, 1)


mlp_input = Input(shape=(X_train_scaled.shape[1],), name='mlp_input')
x = Dense(128, activation='relu')(mlp_input)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.2)(x)
mlp_output = Dense(32, activation='relu')(x)

xgb_input = Input(shape=(1,), name='xgb_input')
cat_input = Input(shape=(1,), name='cat_input')

merged = Concatenate()([mlp_output, xgb_input, cat_input])
z = Dense(64, activation='relu')(merged)
z = Dropout(0.2)(z)
z = Dense(32, activation='relu')(z)
final_output = Dense(1, activation='linear')(z)

model = Model(inputs=[mlp_input, xgb_input, cat_input], outputs=final_output)

model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])


early_stop = EarlyStopping(patience=10, restore_best_weights=True)
history = model.fit(
    [X_train_scaled, xgb_train_features, cat_train_features],
    y_train,
    validation_split=0.1,
    epochs=100,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

y_pred = model.predict([X_test_scaled, xgb_test_features, cat_test_features]).flatten()

r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print("\n--- Hybrid Deep Learning Model Results ---")
print(f"Test R²: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")