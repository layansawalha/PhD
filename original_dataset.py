# -*- coding: utf-8 -*-
"""Original Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SwpPkYK9HCvKyqoHgACIBO8KIQkm7nUg
"""

!pip install transformers torch scikit-learn pandas num2words xgboost

"""Hybrid Dataset (Text + Numbers)

GPT2
"""

import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from transformers import GPT2Tokenizer, GPT2Model
import torch
import xgboost as xgb

df = pd.read_csv("Processed_Datass.csv")

text_columns = ['type_of_work', 'main_construction', 'location']
numeric_columns = ['original_date_factor', 'floor_area', 'storeys', 'building_function_code']
target_column = 'cost_rebased'

df = df[text_columns + numeric_columns + [target_column]].dropna()

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
gpt2_model = GPT2Model.from_pretrained("gpt2")
gpt2_model.eval()

def embed_text(text):
    tokens = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=30)
    with torch.no_grad():
        outputs = gpt2_model(**tokens)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def embed_row(row):
    text = " ".join([str(row[col]) for col in text_columns])
    return embed_text(text)

print("Embedding text data with GPT-2...")
text_embeddings = np.vstack(df[text_columns].apply(embed_row, axis=1))

scaler = StandardScaler()
numeric_data = scaler.fit_transform(df[numeric_columns])

X = np.hstack([text_embeddings, numeric_data])
y = df[target_column].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = xgb.XGBRegressor(n_estimators=150, max_depth=6, learning_rate=0.05, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nEvaluation Metrics:")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"R² Score: {r2:.4f}")

"""GPT2+XGBoost"""

import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from transformers import GPT2Tokenizer, GPT2Model
import torch
import xgboost as xgb

df = pd.read_csv("Processed_Datass.csv")

text_columns = ['type_of_work', 'main_construction', 'location']
numeric_columns = ['original_date_factor', 'floor_area', 'storeys', 'building_function_code']
target_column = 'cost_rebased'

df = df[text_columns + numeric_columns + [target_column]].dropna()

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
gpt2_model = GPT2Model.from_pretrained("gpt2")
gpt2_model.eval()

def embed_text(text):
    tokens = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=30)
    with torch.no_grad():
        outputs = gpt2_model(**tokens)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def embed_row(row):
    text = " ".join([str(row[col]) for col in text_columns])
    return embed_text(text)

print("Embedding text data with GPT-2...")
text_embeddings = np.vstack(df[text_columns].apply(embed_row, axis=1))

scaler = StandardScaler()
numeric_data = scaler.fit_transform(df[numeric_columns])

X = np.hstack([text_embeddings, numeric_data])
y = df[target_column].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

xgb_model = xgb.XGBRegressor(n_estimators=150, max_depth=6, learning_rate=0.05, random_state=42)
xgb_model.fit(X_train, y_train)

y_pred = xgb_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nEvaluation Metrics:")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"R² Score: {r2:.4f}")

def hybrid_predict(row):
    text = " ".join([str(row[col]) for col in text_columns])
    text_embedding = embed_text(text).reshape(1, -1)
    numeric_input = scaler.transform([row[numeric_columns]])
    xgb_input = np.hstack([text_embedding, numeric_input])
    gpt2_prediction = np.mean(text_embedding)
    xgb_prediction = xgb_model.predict(xgb_input)
    return (gpt2_prediction + xgb_prediction) / 2

hybrid_predictions = np.array([hybrid_predict(row) for _, row in df.iterrows()])

rmse_hybrid = np.sqrt(mean_squared_error(y, hybrid_predictions))
mae_hybrid = mean_absolute_error(y, hybrid_predictions)
r2_hybrid = r2_score(y, hybrid_predictions)

print("\nHybrid Model Evaluation Metrics:")
print(f"RMSE: {rmse_hybrid:.4f}")
print(f"MAE: {mae_hybrid:.4f}")
print(f"R² Score: {r2_hybrid:.4f}")

"""Mistral"""

!pip install datasets

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from datasets import Dataset
import pandas as pd
import re
from tqdm import tqdm
from sklearn.model_selection import train_test_split

df = pd.read_csv("Processed_Datass.csv")
text_columns = ['type_of_work', 'main_construction', 'location']
numeric_columns = ['original_date_factor', 'floor_area', 'storeys', 'building_function_code']
target_column = 'cost_rebased'
df = df[text_columns + numeric_columns + [target_column]].dropna()

def format_instruction(row):
    return (
        f"Given the following building information:\n"
        f"- Type of Work: {row['type_of_work']}\n"
        f"- Main Construction: {row['main_construction']}\n"
        f"- Location: {row['location']}\n"
        f"- Original Date Factor: {row['original_date_factor']}\n"
        f"- Floor Area: {row['floor_area']}\n"
        f"- Storeys: {row['storeys']}\n"
        f"- Building Function Code: {row['building_function_code']}\n\n"
        f"What is the cost_rebased?\nAnswer: {row[target_column]}"
    )

df['text'] = df.apply(format_instruction, axis=1)

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

model_name = "mistralai/Mistral-7B-Instruct-v0.1"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

model = get_peft_model(model, peft_config)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/mistralfinetunedcost",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
)

print("Starting training...")
trainer.train()
print("Training completed!")

model.save_pretrained("/content/drive/MyDrive/mistralfinetunedcost")
tokenizer.save_pretrained("/content/drive/MyDrive/mistralfinetunedcost")

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import os
import re
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import pandas as pd
from accelerate import Accelerator
from transformers import BitsAndBytesConfig

model_path = "/content/drive/MyDrive/mistralfinetunedcost"
offload_dir = "./offload"
os.makedirs(offload_dir, exist_ok=True)

accelerator = Accelerator()
device = accelerator.device

tokenizer = AutoTokenizer.from_pretrained(model_path)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

try:
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        offload_folder=offload_dir,
        trust_remote_code=True,
        quantization_config=bnb_config,
    )
except Exception as e:
    print(f"Error loading model with device_map='auto': {e}")
    print("Falling back to loading directly onto GPU...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
    ).to(device)
    if model.device != device:
        print(f"Failed to load model on {device}, check your Colab runtime and GPU.")

model.eval()

df = pd.read_csv("Processed_Datass.csv")
text_columns = ['type_of_work', 'main_construction', 'location']
numeric_columns = ['original_date_factor', 'floor_area', 'storeys', 'building_function_code']
target_column = 'cost_rebased'
df = df[text_columns + numeric_columns + [target_column]].dropna()

from sklearn.model_selection import train_test_split
_, test_df = train_test_split(df, test_size=0.2, random_state=42)

def make_test_prompt(row):
    return (
        f"Given the following building information:\n"
        f"- Type of Work: {row['type_of_work']}\n"
        f"- Main Construction: {row['main_construction']}\n"
        f"- Location: {row['location']}\n"
        f"- Original Date Factor: {row['original_date_factor']}\n"
        f"- Floor Area: {row['floor_area']}\n"
        f"- Storeys: {row['storeys']}\n"
        f"- Building Function Code: {row['building_function_code']}\n\n"
        f"What is the cost_rebased?\nAnswer:"
    )

generated_costs = []
true_costs = []

print("Generating predictions using fine-tuned Mistral...")

for _, row in tqdm(test_df.iterrows(), total=len(test_df)):
    prompt = make_test_prompt(row)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=20,
            do_sample=False,
            temperature=0.7,
        )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    match = re.search(r"\b\d+(?:\.\d+)?\b", result.split("Answer:")[-1])
    pred_cost = float(match.group()) if match else None

    if pred_cost is not None:
        generated_costs.append(pred_cost)
        true_costs.append(row[target_column])

if generated_costs:
    rmse = np.sqrt(mean_squared_error(true_costs, generated_costs))
    mae = mean_absolute_error(true_costs, generated_costs)
    r2 = r2_score(true_costs, generated_costs)

    print("\nFine-Tuned Mistral Evaluation Results:")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R² Score: {r2:.4f}")
else:
    print("No valid predictions were made. Check prompt format and output extraction.")

sample = test_df.iloc[0]
sample_prompt = make_test_prompt(sample)
print("\nExample Prediction Prompt:")
print(sample_prompt)

inputs = tokenizer(sample_prompt, return_tensors="pt", truncation=True).to(device)
outputs = model.generate(**inputs, max_new_tokens=20)
print("\nGenerated Answer:")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

"""deepseek"""

pip install torch transformers peft datasets pandas scikit-learn accelerate

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from datasets import Dataset
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("Processed_Datass.csv")

text_columns = ['type_of_work', 'main_construction', 'location']
numeric_columns = ['original_date_factor', 'floor_area', 'storeys', 'building_function_code']
target_column = 'cost_rebased'

df = df[text_columns + numeric_columns + [target_column]].dropna()

def format_instruction(row):
    return (
        f"Given the following building information:\n"
        f"- Type of Work: {row['type_of_work']}\n"
        f"- Main Construction: {row['main_construction']}\n"
        f"- Location: {row['location']}\n"
        f"- Original Date Factor: {row['original_date_factor']}\n"
        f"- Floor Area: {row['floor_area']}\n"
        f"- Storeys: {row['storeys']}\n"
        f"- Building Function Code: {row['building_function_code']}\n\n"
        f"What is the cost_rebased?\nAnswer: {row[target_column]}"
    )

df['text'] = df.apply(format_instruction, axis=1)

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

model_name = "deepseek-ai/deepseek-coder-7b-instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"]
)

model = get_peft_model(model, peft_config)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/deepseek_finetuned_cost",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
)

print("Starting training with DeepSeek model...")
trainer.train()
print("Training completed!")

model.save_pretrained("/content/drive/MyDrive/deepseek_finetuned_cost")
tokenizer.save_pretrained("/content/drive/MyDrive/deepseek_finetuned_cost")

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from tqdm import tqdm
import pandas as pd
import numpy as np
import re
import os
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from accelerate import Accelerator

model_path = "/content/drive/MyDrive/deepseek_finetuned_cost"
accelerator = Accelerator()
device = accelerator.device

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

try:
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map={"": 0},
        trust_remote_code=True
    ).to(device)
    model.eval()
    print("DeepSeek model loaded onto GPU successfully for evaluation.")
except ValueError as e:
    print(f"ValueError during loading: {e}")
    print("Trying to load with CPU offloading (performance might be slow).")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        llm_int8_enable_fp32_cpu_offload=True
    ).to(device)
    model.eval()
    print("Model loaded with potential CPU offloading.")
except Exception as e:
    print(f"An unexpected error occurred during model loading: {e}")
    raise

df = pd.read_csv("Processed_Datass.csv")
text_columns = ['type_of_work', 'main_construction', 'location']
numeric_columns = ['original_date_factor', 'floor_area', 'storeys', 'building_function_code']
target_column = 'cost_rebased'

df = df[text_columns + numeric_columns + [target_column]].dropna()
_, test_df = train_test_split(df, test_size=0.2, random_state=42)

def make_prompt(row):
    return (
        f"Given the following building information:\n"
        f"- Type of Work: {row['type_of_work']}\n"
        f"- Main Construction: {row['main_construction']}\n"
        f"- Location: {row['location']}\n"
        f"- Original Date Factor: {row['original_date_factor']}\n"
        f"- Floor Area: {row['floor_area']}\n"
        f"- Storeys: {row['storeys']}\n"
        f"- Building Function Code: {row['building_function_code']}\n\n"
        f"What is the cost_rebased?\nAnswer:"
    )

generated_costs = []
true_costs = []

print("Generating predictions with fine-tuned DeepSeek model...")

for _, row in tqdm(test_df.iterrows(), total=len(test_df)):
    prompt = make_prompt(row)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=20,
            do_sample=False,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id if tokenizer.pad_token_id is not None else tokenizer.unk_token_id
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    match = re.search(r"\b\d+(?:\.\d+)?\b", decoded.split("Answer:")[-1])
    pred = float(match.group()) if match else None

    if pred is not None:
        generated_costs.append(pred)
        true_costs.append(row[target_column])

if generated_costs:
    rmse = np.sqrt(mean_squared_error(true_costs, generated_costs))
    mae = mean_absolute_error(true_costs, generated_costs)
    r2 = r2_score(true_costs, generated_costs)

    print("\n--- Fine-Tuned DeepSeek Model Evaluation Results ---")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R² Score: {r2:.4f}")
else:
    print("No predictions extracted. Please review the prompt or model output.")

sample = test_df.iloc[0]
prompt = make_prompt(sample)
print("\n--- Example Prompt ---")
print(prompt)

inputs = tokenizer(prompt, return_tensors="pt").to(device)
output = model.generate(**inputs, max_new_tokens=20)
print("\n--- Model Response ---")
print(tokenizer.decode(output[0], skip_special_tokens=True))

"""optimised gpt2"""

import os
os.environ['OPENAI_API_KEY'] = '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'

"""deepseek"""

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from datasets import Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import re
from sklearn.metrics import mean_squared_error, r2_score
import os

df = pd.read_csv("Processed_Datass.csv")
text_columns = ['type_of_work', 'main_construction', 'location']
numeric_columns = ['original_date_factor', 'floor_area', 'storeys', 'building_function_code']
target_column = 'cost_rebased'
df = df[text_columns + numeric_columns + [target_column]].dropna()

def format_instruction(row):
    return (
        f"Given the following building information:\n"
        f"- Type of Work: {row['type_of_work']}\n"
        f"- Main Construction: {row['main_construction']}\n"
        f"- Location: {row['location']}\n"
        f"- Original Date Factor: {row['original_date_factor']}\n"
        f"- Floor Area: {row['floor_area']}\n"
        f"- Storeys: {row['storeys']}\n"
        f"- Building Function Code: {row['building_function_code']}\n\n"
        f"What is the cost_rebased?\nAnswer: {row[target_column]}"
    )

df['text'] = df.apply(format_instruction, axis=1)

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

model_name = "deepseek-ai/deepseek-coder-7b-instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"]
)

model = get_peft_model(model, peft_config)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

output_dir = "/content/drive/MyDrive/deepseek_finetuned_cost"
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
)

print("Starting training with DeepSeek model...")
trainer.train()
print("Training completed!")

model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print("\n--- Evaluating Fine-Tuned DeepSeek Model ---")

def make_prompt_deepseek_finetuned(row):
    return (
        f"Given the following building information:\n"
        f"- Type of Work: {row['type_of_work']}\n"
        f"- Main Construction: {row['main_construction']}\n"
        f"- Location: {row['location']}\n"
        f"- Original Date Factor: {row['original_date_factor']}\n"
        f"- Floor Area: {row['floor_area']}\n"
        f"- Storeys: {row['storeys']}\n"
        f"- Building Function Code: {row['building_function_code']}\n\n"
        f"What is the cost_rebased?\nAnswer:"
    )

generated_costs_finetuned = []
true_costs_finetuned = []

model.eval()
device = model.device

for _, row in test_df.iterrows():
    prompt = make_prompt_deepseek_finetuned(row)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=20,
            do_sample=False,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id if tokenizer.pad_token_id is not None else tokenizer.unk_token_id
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    match = re.search(r"\b\d+(?:\.\d+)?\b", decoded.split("Answer:")[-1])
    pred = float(match.group()) if match else None

    if pred is not None:
        generated_costs_finetuned.append(pred)
        true_costs_finetuned.append(row[target_column])

if generated_costs_finetuned:
    rmse_finetuned = np.sqrt(mean_squared_error(true_costs_finetuned, generated_costs_finetuned))
    r2_finetuned = r2_score(true_costs_finetuned, generated_costs_finetuned)

    print("\n--- Fine-Tuned DeepSeek Model Evaluation Results ---")
    print(f"RMSE: {rmse_finetuned:.4f}")
    print(f"R² Score: {r2_finetuned:.4f}")
else:
    print("No predictions extracted from fine-tuned DeepSeek model. Please review the prompt or model output.")

sample_finetuned = test_df.iloc[0]
prompt_finetuned = make_prompt_deepseek_finetuned(sample_finetuned)
print("\n--- Fine-Tuned DeepSeek Example Prompt ---")
print(prompt_finetuned)

inputs_finetuned = tokenizer(prompt_finetuned, return_tensors="pt").to(device)
with torch.no_grad():
    output_finetuned = model.generate(**inputs_finetuned, max_new_tokens=20)
print("\n--- Fine-Tuned DeepSeek Model Response ---")
print(tokenizer.decode(output_finetuned[0], skip_special_tokens=True))

"""ML Models"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load your data
df = pd.read_csv('/content/Processed_Data.csv')

input_cols = ['type_of_work', 'building_function_code', 'original_date_factor',
              'floor_area', 'main_construction', 'storeys', 'original_location_factor']
target_col = 'cost_rebased'

X = df[input_cols]
y = df[target_col]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

poly = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regression': DecisionTreeRegressor(random_state=42),
    'Random Forest Regression': RandomForestRegressor(random_state=42),
    'Gradient Boosting Regression': GradientBoostingRegressor(random_state=42)
}


rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}
rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, cv=5, scoring='neg_mean_squared_error')

gb_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.5],
    'max_depth': [3, 5, 7]
}
gb_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_param_grid, cv=5, scoring='neg_mean_squared_error')

def evaluate_model(name, model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"{name}:\n RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.3f}\n")

for name, model in models.items():
    if name in ['Random Forest Regression', 'Gradient Boosting Regression']:
        print(f"Evaluating {name} with hyperparameter tuning:")
        if name == 'Random Forest Regression':
            rf_grid_search.fit(X_train_poly, y_train)
            best_rf_model = rf_grid_search.best_estimator_
            evaluate_model(f'Best {name}', best_rf_model, X_train_poly, y_train, X_test_poly, y_test)

        if name == 'Gradient Boosting Regression':
            gb_grid_search.fit(X_train_poly, y_train)
            best_gb_model = gb_grid_search.best_estimator_
            evaluate_model(f'Best {name}', best_gb_model, X_train_poly, y_train, X_test_poly, y_test)
    else:
        # For other models, use the original scaled data
        if 'Tree' in name or 'Forest' in name or 'Boosting' in name:
            evaluate_model(name, model, X_train_poly, y_train, X_test_poly, y_test)
        else:
            evaluate_model(name, model, X_train_scaled, y_train, X_test_scaled, y_test)

pip install lightgbm

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

df = pd.read_csv("/content/Processed_Data.csv")

input_cols = ['type_of_work', 'building_function_code', 'original_date_factor',
              'floor_area', 'main_construction', 'storeys', 'original_location_factor']
target_col = 'cost_rebased'

X = df[input_cols]
y = df[target_col]

categorical_cols = ['type_of_work', 'main_construction', 'original_location_factor']
numeric_cols = [col for col in input_cols if col not in categorical_cols]

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
        ('num', 'passthrough', numeric_cols)
    ]
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# XGBoost
xgb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor(random_state=42))
])
xgb_pipeline.fit(X_train, y_train)

# LightGBM
lgbm_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LGBMRegressor(random_state=42))
])
lgbm_pipeline.fit(X_train, y_train)

def evaluate_model(name, y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"{name} Results:")
    print(f" RMSE: {rmse:.2f}")
    print(f" MAE:  {mae:.2f}")
    print(f" R²:   {r2:.3f}")
    print("-" * 40)

xgb_preds = xgb_pipeline.predict(X_test)
lgbm_preds = lgbm_pipeline.predict(X_test)

evaluate_model("XGBoost", y_test, xgb_preds)
evaluate_model("LightGBM", y_test, lgbm_preds)

"""engression"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


try:
    df = pd.read_csv("Processed_Data.csv")
    print("Loaded Processed_Data.csv successfully.")


input_cols = ['type_of_work', 'building_function_code', 'original_date_factor',
              'floor_area', 'main_construction', 'storeys', 'original_location_factor']
target_col = 'cost_rebased'

X = df[input_cols]
y = df[target_col]

for col in X.columns:
    if X[col].dtype == 'object' or pd.api.types.is_categorical_dtype(X[col]):
        X[col] = X[col].fillna(X[col].mode()[0])
    elif pd.api.types.is_numeric_dtype(X[col]):
        X[col] = X[col].fillna(X[col].mean())
y = y.fillna(y.mean())


categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
numeric_cols = X.select_dtypes(include=np.number).columns.tolist()

if 'building_function_code' in numeric_cols and 'building_function_code' not in categorical_cols:
    pass
elif 'building_function_code' in input_cols and 'building_function_code' not in (categorical_cols + numeric_cols):
    if pd.api.types.is_numeric_dtype(df['building_function_code']):
        numeric_cols.append('building_function_code')
    else:
        categorical_cols.append('building_function_code')
    numeric_cols = list(set(numeric_cols))
    categorical_cols = list(set(categorical_cols))


preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),
        ('num', StandardScaler(), numeric_cols)
    ],
    remainder='passthrough'
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)
y_train_values = y_train.values
y_test_values = y_test.values


def build_conceptual_engression_model(input_shape):
    """Builds a Keras model for regression."""
    model = keras.Sequential([
        layers.Dense(128, activation='relu', input_shape=[input_shape], kernel_regularizer=keras.regularizers.l2(0.001)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(32, activation='relu'),
        layers.Dense(1)
    ])

    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

    model.compile(loss='mean_squared_error',
                  optimizer=optimizer,
                  metrics=['mean_absolute_error'])
    return model

input_shape_nn = X_train_processed.shape[1]
if input_shape_nn == 0:
    raise ValueError("Input shape for NN is 0. Check preprocessing and feature columns.")

conceptual_engression_model = build_conceptual_engression_model(input_shape_nn)
print("\n--- Conceptual Engression (Neural Network) ---")
print(conceptual_engression_model.summary())

early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)

print("Training Conceptual Engression model...")
history = conceptual_engression_model.fit(
    X_train_processed, y_train_values,
    epochs=150,
    validation_split=0.2,
    verbose=1,
    callbacks=[early_stopping, reduce_lr]
)

print("\nEvaluating Conceptual Engression model...")
loss, mae = conceptual_engression_model.evaluate(X_test_processed, y_test_values, verbose=0)
y_pred_nn = conceptual_engression_model.predict(X_test_processed).flatten()

rmse_nn = np.sqrt(mean_squared_error(y_test_values, y_pred_nn))
r2_nn = r2_score(y_test_values, y_pred_nn)

print("Conceptual Engression (Neural Network) Results:")
print(f" Test Loss (MSE): {loss:.2f}")
print(f" RMSE: {rmse_nn:.2f}")
print(f" MAE:  {mae:.2f}")
print(f" R²:   {r2_nn:.3f}")
print("-" * 50)

"""LASER"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import statsmodels.api as sm

try:
    df = pd.read_csv("Processed_Data.csv")
    print("Loaded Processed_Data.csv successfully.")
except FileNotFoundError:
    print("Processed_Data.csv not found. Creating a dummy dataset for demonstration.")
    data = {
        'type_of_work': np.random.choice(['New', 'Repair', 'Renovation', 'Alteration'], 1000),
        'building_function_code': np.random.randint(100, 999, 1000), # Treated as numeric
        'original_date_factor': np.random.rand(1000) * 10 + 1,
        'floor_area': np.random.rand(1000) * 2000 + 50,
        'main_construction': np.random.choice(['Brick', 'Concrete', 'Steel', 'Wood', 'Other'], 1000),
        'storeys': np.random.randint(1, 20, 1000),
        'original_location_factor': np.random.choice(['Urban', 'Suburban', 'Rural', 'Industrial'], 1000),
        'cost_rebased': np.random.rand(1000) * 500000 + 100000
    }
    df = pd.DataFrame(data)
    df.to_csv("Processed_Data.csv", index=False)
    print("Dummy Processed_Data.csv created and saved.")

input_cols = ['type_of_work', 'building_function_code', 'original_date_factor',
              'floor_area', 'main_construction', 'storeys', 'original_location_factor']
target_col = 'cost_rebased'

X = df[input_cols].copy()
y = df[target_col].copy()

# Handle potential NaN values
for col in X.columns:
    if X[col].dtype == 'object' or pd.api.types.is_categorical_dtype(X[col]):
        X[col].fillna(X[col].mode()[0], inplace=True)
    elif pd.api.types.is_numeric_dtype(X[col]):
        X[col].fillna(X[col].mean(), inplace=True)
y.fillna(y.mean(), inplace=True)


categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
numeric_cols = X.select_dtypes(include=np.number).columns.tolist()

# Adjust categorical/numeric lists if necessary
if 'building_function_code' in numeric_cols and 'building_function_code' not in categorical_cols:
    pass
elif 'building_function_code' in input_cols and 'building_function_code' not in (categorical_cols + numeric_cols):
    if pd.api.types.is_numeric_dtype(df['building_function_code']):
        if 'building_function_code' not in numeric_cols: numeric_cols.append('building_function_code')
    else:
        if 'building_function_code' not in categorical_cols: categorical_cols.append('building_function_code')
numeric_cols = list(set(numeric_cols))
categorical_cols = list(set(categorical_cols))


preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),
        ('num', StandardScaler(), numeric_cols)
    ],
    remainder='passthrough'
)

# --- 3. Train-test split ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

try:

    feature_names_out = preprocessor.get_feature_names_out()
except AttributeError:
    ohe_feature_names = []
    cat_transformer = preprocessor.named_transformers_['cat']
    if hasattr(cat_transformer, 'categories_'):
        for i, col_name in enumerate(categorical_cols):
            categories = cat_transformer.categories_[i]
            ohe_feature_names.extend([f"{col_name}_{cat}" for cat in categories])
    else:
         for col_name in categorical_cols: ohe_feature_names.append(f"{col_name}_unknown")

    num_transformer_cols = []
    if 'num' in preprocessor.named_transformers_ and preprocessor.named_transformers_['num'] != 'drop':
        num_transformer_cols = numeric_cols
    feature_names_out = ohe_feature_names + num_transformer_cols

X_train_processed_df = pd.DataFrame(X_train_processed, columns=feature_names_out)
X_test_processed_df = pd.DataFrame(X_test_processed, columns=feature_names_out)



selected_feature_name = None
if 'num__floor_area' in X_train_processed_df.columns:
    selected_feature_name = 'num__floor_area'
elif 'floor_area' in X_train_processed_df.columns:
    selected_feature_name = 'floor_area'
else:
    processed_numeric_cols = [col for col in X_train_processed_df.columns if any(num_col_original in col for num_col_original in numeric_cols)]
    if processed_numeric_cols:
        selected_feature_name = processed_numeric_cols[0]

print("\n--- LASER (Conceptual using LOWESS on one feature) ---")
if selected_feature_name and selected_feature_name in X_train_processed_df.columns:
    print(f"Using feature '{selected_feature_name}' for LOWESS demonstration.")

    x_train_lowess = X_train_processed_df[selected_feature_name].values
    x_test_lowess = X_test_processed_df[selected_feature_name].values
    y_train_lowess = y_train.values
    y_test_lowess = y_test.values
  sorted_indices_train = np.argsort(x_train_lowess)
    x_train_sorted = x_train_lowess[sorted_indices_train]
    y_train_sorted = y_train_lowess[sorted_indices_train]


    lowess_results = sm.nonparametric.lowess(y_train_sorted, x_train_sorted, frac=0.3, it=0)

    min_train_x = lowess_results[:, 0].min()
    max_train_x = lowess_results[:, 0].max()
    x_test_lowess_clipped = np.clip(x_test_lowess, min_train_x, max_train_x)

    y_pred_lowess_test = np.interp(x_test_lowess_clipped, lowess_results[:, 0], lowess_results[:, 1])

    rmse_lowess = np.sqrt(mean_squared_error(y_test_lowess, y_pred_lowess_test))
    mae_lowess = mean_absolute_error(y_test_lowess, y_pred_lowess_test)
    r2_lowess = r2_score(y_test_lowess, y_pred_lowess_test)

    print(f"LOWESS (on processed '{selected_feature_name}') Results:")
    print(f" RMSE: {rmse_lowess:.2f}")
    print(f" MAE:  {mae_lowess:.2f}")
    print(f" R²:   {r2_lowess:.3f}")
else:
    print(f"Could not find a suitable numeric feature for LOWESS example. Searched for variants of 'floor_area' or first numeric.")
    print(f"Available processed columns: {X_train_processed_df.columns.tolist()}")
print("-" * 50)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


try:
    df = pd.read_csv("Processed_Data.csv")
    print("Loaded Processed_Data.csv successfully.")



input_cols = ['type_of_work', 'building_function_code', 'original_date_factor',
              'floor_area', 'main_construction', 'storeys', 'original_location_factor']
target_col = 'cost_rebased'

X = df[input_cols]
y = df[target_col]

for col in X.columns:
    if X[col].dtype == 'object' or pd.api.types.is_categorical_dtype(X[col]):
        X[col] = X[col].fillna(X[col].mode()[0])
    elif pd.api.types.is_numeric_dtype(X[col]):
        X[col] = X[col].fillna(X[col].mean())
y = y.fillna(y.mean())

categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
numeric_cols = X.select_dtypes(include=np.number).columns.tolist()

if 'building_function_code' in numeric_cols and 'building_function_code' not in categorical_cols:
    pass
elif 'building_function_code' in input_cols and 'building_function_code' not in (categorical_cols + numeric_cols):
    if pd.api.types.is_numeric_dtype(df['building_function_code']):
        if 'building_function_code' not in numeric_cols: numeric_cols.append('building_function_code')
    else:
        if 'building_function_code' not in categorical_cols: categorical_cols.append('building_function_code')
numeric_cols = list(set(numeric_cols))
categorical_cols = list(set(categorical_cols))


preprocessor_lstm = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),
        ('num', StandardScaler(), numeric_cols)
    ],
    remainder='passthrough'
)

X_processed_full = preprocessor_lstm.fit_transform(X)
y_values_full = y.values.reshape(-1, 1)

def create_sequences(features, target, n_past, n_future, train_test_ratio=0.8):
    """
    Creates sequences from time series data and splits them into train/test.
    """
    X_seq, y_seq = [], []
    for i in range(n_past, len(features) - n_future + 1):
        X_seq.append(features[i - n_past:i, :])
        y_seq.append(target[i + n_future - 1 : i + n_future, 0])

    X_seq, y_seq = np.array(X_seq), np.array(y_seq)

    if X_seq.shape[0] == 0:
        return None, None, None, None, 0, 0

    split_idx = int(len(X_seq) * train_test_ratio)
    X_train_seq, X_test_seq = X_seq[:split_idx], X_seq[split_idx:]
    y_train_seq, y_test_seq = y_seq[:split_idx], y_seq[split_idx:]

    n_features = features.shape[1]

    return X_train_seq, X_test_seq, y_train_seq, y_test_seq, n_features, X_seq.shape[0]


n_past = 20
n_future = 1

X_train_seq, X_test_seq, y_train_seq, y_test_seq, n_features_lstm, total_sequences = create_sequences(
    X_processed_full, y_values_full, n_past, n_future, train_test_ratio=0.8
)

print("\n--- Deep Learning for Time Series (LSTM) ---")

if total_sequences == 0 or X_train_seq is None or X_train_seq.shape[0] == 0 or X_test_seq.shape[0] == 0 :
    print(f"Not enough data to create sequences. Needed at least {n_past + n_future -1} data points. Got {len(X_processed_full)} after processing.")
    print("Skipping LSTM model training and evaluation.")
else:
    print(f"Created {X_train_seq.shape[0]} training sequences and {X_test_seq.shape[0]} testing sequences.")
    print(f"Input shape for LSTM: (samples, timesteps={n_past}, features={n_features_lstm})")

    lstm_model = keras.Sequential([
        layers.LSTM(64, activation='relu', input_shape=(n_past, n_features_lstm), return_sequences=True), # More complex LSTM
        layers.Dropout(0.25),
        layers.LSTM(32, activation='relu', return_sequences=False),
        layers.Dropout(0.25),
        layers.Dense(16, activation='relu'),
        layers.Dense(n_future)
    ])

    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    lstm_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    print(lstm_model.summary())

    early_stopping_lstm = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    reduce_lr_lstm = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)

    print("Training LSTM model...")
    history_lstm = lstm_model.fit(
        X_train_seq, y_train_seq,
        epochs=100,
        validation_data=(X_test_seq, y_test_seq),
        verbose=1,
        callbacks=[early_stopping_lstm, reduce_lr_lstm]
    )

    print("\nEvaluating LSTM model...")
    loss_lstm, mae_lstm_eval = lstm_model.evaluate(X_test_seq, y_test_seq, verbose=0)
    y_pred_lstm = lstm_model.predict(X_test_seq)

    rmse_lstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_lstm))
    r2_lstm = r2_score(y_test_seq, y_pred_lstm)

    print("LSTM Results:")
    print(f" Test Loss (MSE): {loss_lstm:.2f}")
    print(f" RMSE: {rmse_lstm:.2f}")
    print(f" MAE:  {mae_lstm_eval:.2f}")
    print(f" R²:   {r2_lstm:.3f}")
print("-" * 50)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from xgboost import XGBRegressor

df = pd.read_csv("/content/Finaldataset9.csv")

input_cols = ['type_of_work', 'building_function_code',
              'floor_area', 'main_construction', 'storeys']
target_col = 'cost_rebased'
X = df[input_cols]
y = df[target_col]

categorical_cols = ['type_of_work', 'main_construction']
numeric_cols = [col for col in input_cols if col not in categorical_cols]


preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),
    ('num', StandardScaler(), numeric_cols)
])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import torch
import torch.nn as nn

class DeepRegressor:
    def __init__(self, input_dim, epochs=100, lr=0.01):
        self.input_dim = input_dim
        self.epochs = epochs
        self.lr = lr

    def fit(self, X, y):
        X_tensor = torch.tensor(X.toarray() if hasattr(X, "toarray") else X, dtype=torch.float32)
        y_tensor = torch.tensor(y.values if isinstance(y, pd.Series) else y, dtype=torch.float32).view(-1, 1)

        self.model = nn.Sequential(
            nn.Linear(self.input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        loss_fn = nn.MSELoss()

        for _ in range(self.epochs):
            self.model.train()
            optimizer.zero_grad()
            pred = self.model(X_tensor)
            loss = loss_fn(pred, y_tensor)
            loss.backward()
            optimizer.step()
        return self

    def predict(self, X):
        X_tensor = torch.tensor(X.toarray() if hasattr(X, "toarray") else X, dtype=torch.float32)
        with torch.no_grad():
            return self.model(X_tensor).numpy().flatten()

from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.neighbors import KNeighborsRegressor
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted

class LASERRegressor(BaseEstimator, RegressorMixin):
    def __init__(self, base_model=None, k_neighbors=10):
        self.k_neighbors = k_neighbors
        self.base_model = base_model if base_model is not None else HuberRegressor()

    def fit(self, X, y):
        X, y = check_X_y(X, y)
        self.knn = KNeighborsRegressor(n_neighbors=self.k_neighbors)
        self.knn.fit(X, y)
        self.base_model.fit(X, y)
        self.X_ = X
        return self

    def predict(self, X):
        check_is_fitted(self)
        X = check_array(X)
        weights = 1 / (1 + np.linalg.norm(X[:, None] - self.X_, axis=2))
        pred = self.base_model.predict(X)
        return pred

class EngressionRegressor(BaseEstimator, RegressorMixin):
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100)

    def fit(self, X, y):
        self.model.fit(X, y)
        return self

    def predict(self, X):
        return self.model.predict(X)

def evaluate_model(name, model):
    if name == "Deep Learning (FCNN)":
        X_train_transformed = preprocessor.fit_transform(X_train)
        X_test_transformed = preprocessor.transform(X_test)
        model.fit(X_train_transformed, y_train)
        y_pred = model.predict(X_test_transformed)
    else:
        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', model)])
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"{name} Results:")
    print(f" RMSE: {rmse:.2f}")
    print(f" MAE: {mae:.2f}")
    print(f" R²: {r2:.3f}")
    print("-" * 50)

models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(max_iter=10000),
    'SVR': SVR(),
    'Decision Tree Regression': DecisionTreeRegressor(random_state=42),
    'Random Forest Regression': RandomForestRegressor(random_state=42),
    'Robust Linear (Huber)': HuberRegressor(),
    'Gaussian Process Regression': GaussianProcessRegressor(kernel=C(1.0) * RBF(10), n_restarts_optimizer=5),
    'XGBoost (LSBoost)': XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1),
    'Deep Learning (FCNN)': DeepRegressor(input_dim=preprocessor.fit_transform(X_train).shape[1]),
    'LASER (Locally Adaptive)': LASERRegressor(),
    'Engression (Mocked RF)': EngressionRegressor()
}

for name, model in models.items():
    evaluate_model(name, model)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import HuberRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader

df = pd.read_csv("Processed_Data.csv")

input_cols = ['type_of_work', 'building_function_code', 'original_date_factor',
              'floor_area', 'main_construction', 'storeys', 'original_location_factor']
target_col = 'cost_rebased'

X = df[input_cols]
y = df[target_col]

categorical_cols = ['type_of_work', 'main_construction', 'original_location_factor']
numeric_cols = [col for col in input_cols if col not in categorical_cols]

preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
    ('num', StandardScaler(), numeric_cols)
])

X_processed = preprocessor.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

class Compressor(nn.Module):
    def __init__(self, input_dim, latent_dim=16):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        z = self.encoder(x)
        y_hat = self.decoder(z)
        return y_hat, z

compressor = Compressor(input_dim=X_train_tensor.shape[1])
optimizer = torch.optim.Adam(compressor.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

for epoch in range(100):
    compressor.train()
    optimizer.zero_grad()
    y_pred, _ = compressor(X_train_tensor)
    loss = loss_fn(y_pred, y_train_tensor)
    loss.backward()
    optimizer.step()

compressor.eval()
with torch.no_grad():
    _, z_train = compressor(X_train_tensor)
    _, z_test = compressor(X_test_tensor)

z_train_np = z_train.numpy()
z_test_np = z_test.numpy()

huber = HuberRegressor().fit(X_train, y_train)
gpr = GaussianProcessRegressor(kernel=C(1.0) * RBF(10), n_restarts_optimizer=5)
gpr.fit(X_train[:1000].toarray(), y_train[:1000])
xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)
xgb.fit(X_train, y_train)

pred_train_huber = huber.predict(X_train)
pred_train_gpr = gpr.predict(X_train[:1000].toarray())
pred_train_xgb = xgb.predict(X_train)

pred_test_huber = huber.predict(X_test)
pred_test_gpr = gpr.predict(X_test[:1000].toarray())
pred_test_xgb = xgb.predict(X_test)

def pad_predictions(pred, length):
    pad = np.mean(pred) * np.ones(length)
    pad[:len(pred)] = pred
    return pad

pred_train_gpr = pad_predictions(pred_train_gpr, len(y_train))
pred_test_gpr = pad_predictions(pred_test_gpr, len(y_test))

class GatingNet(nn.Module):
    def __init__(self, input_dim, n_models=3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, n_models)
        )

    def forward(self, x):
        return F.softmax(self.net(x), dim=1)

gating_net = GatingNet(input_dim=z_train.shape[1])
optimizer = torch.optim.Adam(gating_net.parameters(), lr=0.01)

base_preds_train = np.stack([pred_train_huber, pred_train_gpr, pred_train_xgb], axis=1)
base_preds_test = np.stack([pred_test_huber, pred_test_gpr, pred_test_xgb], axis=1)

z_train_tensor = torch.tensor(z_train_np, dtype=torch.float32)
z_test_tensor = torch.tensor(z_test_np, dtype=torch.float32)
base_preds_train_tensor = torch.tensor(base_preds_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)

for epoch in range(100):
    gating_net.train()
    optimizer.zero_grad()
    weights = gating_net(z_train_tensor)
    weighted_preds = torch.sum(weights * base_preds_train_tensor, dim=1, keepdim=True)
    loss = F.mse_loss(weighted_preds, y_train_tensor)
    loss.backward()
    optimizer.step()

gating_net.eval()
with torch.no_grad():
    weights_test = gating_net(z_test_tensor)
    base_preds_test_tensor = torch.tensor(base_preds_test, dtype=torch.float32)
    final_preds = torch.sum(weights_test * base_preds_test_tensor, dim=1).numpy()

rmse = np.sqrt(mean_squared_error(y_test, final_preds))
mae = mean_absolute_error(y_test, final_preds)
r2 = r2_score(y_test, final_preds)

print("Final Ensemble Results:")
print(f" RMSE: {rmse:.2f}")
print(f" MAE: {mae:.2f}")
print(f" R²: {r2:.3f}")

"""Hybrid Quantum-Classical Regression"""

import pennylane as qml
from pennylane import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, TensorDataset

df = pd.read_csv("/content/Processed_Data.csv")


X = df[['type_of_work', 'building_function_code', 'original_date_factor',
        'floor_area', 'main_construction', 'storeys',
        'original_location_factor']].values
y = df['cost_rebased'].values.reshape(-1, 1)

scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()
X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y)

X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_scaled, test_size=0.10, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=42)

train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                         torch.tensor(y_train, dtype=torch.float32))
val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                       torch.tensor(y_val, dtype=torch.float32))
test_ds = TensorDataset(torch.tensor(X_test, dtype=torch.float32),
                        torch.tensor(y_test, dtype=torch.float32))

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=32)

n_qubits = 4
dev = qml.device("lightning.qubit", wires=n_qubits)

def qcircuit(inputs, weights):
    for i, feature in enumerate(inputs):
        qml.RY(feature, wires=i % n_qubits)
    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))
    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

weight_shapes = {"weights": (3, n_qubits, 3)}

@qml.qnode(dev, interface="torch", diff_method="parameter-shift")
def qnode(inputs, weights):
    return qcircuit(inputs, weights)

class HybridRegressor(nn.Module):
    def __init__(self):
        super().__init__()
        self.embed = nn.Linear(7, n_qubits)
        self.qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)
        self.fc1 = nn.Linear(n_qubits, 16)
        self.drop = nn.Dropout(0.1)
        self.fc2 = nn.Linear(16, 1)
        nn.init.xavier_uniform_(self.embed.weight)
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)

    def forward(self, x):
        x = torch.relu(self.embed(x))
        x = torch.stack([self.qlayer(xi) for xi in x])
        x = torch.relu(self.fc1(x))
        x = self.drop(x)
        return self.fc2(x)

model = HybridRegressor()
optimizer = optim.Adam(model.parameters(), lr=5e-4)
loss_fn = nn.MSELoss()

EPOCHS = 400
patience = 10
best_val_loss = float('inf')
epochs_no_improve = 0

for epoch in range(1, EPOCHS + 1):
    model.train()
    train_losses = []
    for Xb, yb in train_loader:
        optimizer.zero_grad()
        preds = model(Xb).reshape(yb.shape)
        loss = loss_fn(preds, yb)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())


    model.eval()
    val_losses = []
    with torch.no_grad():
        for Xb, yb in val_loader:
            preds = model(Xb).reshape(yb.shape)
            loss = loss_fn(preds, yb)
            val_losses.append(loss.item())

    avg_train_loss = sum(train_losses) / len(train_losses)
    avg_val_loss = sum(val_losses) / len(val_losses)

    print(f"Epoch {epoch}/{EPOCHS} - Train Loss: {avg_train_loss:.4e} - Val Loss: {avg_val_loss:.4e}")

    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), "best_model.pt")
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1

    if epochs_no_improve >= patience:
        print(f"Early stopping at epoch {epoch}. Best Val Loss: {best_val_loss:.4e}")
        break

model.load_state_dict(torch.load("best_model.pt"))

model.eval()
with torch.no_grad():
    X_all = torch.tensor(X_test, dtype=torch.float32)
    y_all = model(X_all).numpy().flatten()

y_pred = scaler_y.inverse_transform(y_all.reshape(-1, 1)).flatten()
y_true = scaler_y.inverse_transform(y_test).flatten()

print("MAE:", mean_absolute_error(y_true, y_pred))
print("RMSE:", (mean_squared_error(y_true, y_pred)) ** 0.5)
print("R**2:", r2_score(y_true, y_pred))

import pennylane as qml
from pennylane import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, TensorDataset

df = pd.read_csv("/content/Finaldataset9.csv")

input_cols = ['type_of_work', 'building_function_code',
              'floor_area', 'main_construction', 'storeys']
target_col = 'cost_rebased'

scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()
X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y)

X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_scaled, test_size=0.10, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=42)

train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                         torch.tensor(y_train, dtype=torch.float32))
val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                       torch.tensor(y_val, dtype=torch.float32))
test_ds = TensorDataset(torch.tensor(X_test, dtype=torch.float32),
                        torch.tensor(y_test, dtype=torch.float32))

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=32)

n_qubits = 4
dev = qml.device("lightning.qubit", wires=n_qubits)

def qcircuit(inputs, weights):
    for i, feature in enumerate(inputs):
        qml.RY(feature, wires=i % n_qubits)
    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))
    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

weight_shapes = {"weights": (3, n_qubits, 3)}

@qml.qnode(dev, interface="torch", diff_method="parameter-shift")
def qnode(inputs, weights):
    return qcircuit(inputs, weights)

class HybridRegressor(nn.Module):
    def __init__(self):
        super().__init__()
        self.embed = nn.Linear(7, n_qubits)
        self.qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)
        self.fc1 = nn.Linear(n_qubits, 16)
        self.drop = nn.Dropout(0.1)
        self.fc2 = nn.Linear(16, 1)
        nn.init.xavier_uniform_(self.embed.weight)
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)

    def forward(self, x):
        x = torch.relu(self.embed(x))
        x = torch.stack([self.qlayer(xi) for xi in x])
        x = torch.relu(self.fc1(x))
        x = self.drop(x)
        return self.fc2(x)

model = HybridRegressor()
optimizer = optim.Adam(model.parameters(), lr=5e-4)
loss_fn = nn.MSELoss()

EPOCHS = 400
patience = 10
best_val_loss = float('inf')
epochs_no_improve = 0

for epoch in range(1, EPOCHS + 1):
    model.train()
    train_losses = []
    for Xb, yb in train_loader:
        optimizer.zero_grad()
        preds = model(Xb).reshape(yb.shape)
        loss = loss_fn(preds, yb)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())


    model.eval()
    val_losses = []
    with torch.no_grad():
        for Xb, yb in val_loader:
            preds = model(Xb).reshape(yb.shape)
            loss = loss_fn(preds, yb)
            val_losses.append(loss.item())

    avg_train_loss = sum(train_losses) / len(train_losses)
    avg_val_loss = sum(val_losses) / len(val_losses)

    print(f"Epoch {epoch}/{EPOCHS} - Train Loss: {avg_train_loss:.4e} - Val Loss: {avg_val_loss:.4e}")

    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), "best_model.pt")
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1

    if epochs_no_improve >= patience:
        print(f"Early stopping at epoch {epoch}. Best Val Loss: {best_val_loss:.4e}")
        break

model.load_state_dict(torch.load("best_model.pt"))

model.eval()
with torch.no_grad():
    X_all = torch.tensor(X_test, dtype=torch.float32)
    y_all = model(X_all).numpy().flatten()

y_pred = scaler_y.inverse_transform(y_all.reshape(-1, 1)).flatten()
y_true = scaler_y.inverse_transform(y_test).flatten()

print("MAE:", mean_absolute_error(y_true, y_pred))
print("RMSE:", (mean_squared_error(y_true, y_pred)) ** 0.5)
print("R**2:", r2_score(y_true, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt

corr = df[input_cols + [target_col]].corr()

sns.heatmap(corr[[target_col]].sort_values(by=target_col, ascending=False), annot=True, cmap='coolwarm')
plt.title(" Correlation of Each Feature with cost_rebased")
plt.show()

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X, y)

importances = rf.feature_importances_
feat_importance = pd.Series(importances, index=input_cols).sort_values(ascending=True)


feat_importance.plot(kind='barh', color='teal')
plt.title(" Feature Importance (Random Forest)")
plt.xlabel("Importance Score")
plt.show()

from sklearn.inspection import permutation_importance
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("model", RandomForestRegressor(n_estimators=100, random_state=42))
])
pipe.fit(X, y)

result = permutation_importance(pipe, X, y, n_repeats=10, random_state=42)

perm_df = pd.Series(result.importances_mean, index=input_cols).sort_values(ascending=True)
perm_df.plot(kind='barh', color='coral')
plt.title(" Permutation Feature Importance")
plt.xlabel("Decrease in Model Score When Shuffled")
plt.show()